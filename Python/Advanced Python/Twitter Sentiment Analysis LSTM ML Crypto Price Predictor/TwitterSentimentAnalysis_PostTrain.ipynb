{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import concatenate\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil\n",
    "import dateutil.parser\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize']=20,10\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import re\n",
    "import oauth2 as oauth\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates\n",
    "import matplotlib.dates as mdates # Formatting dates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler # This Scaler removes the median and scales the data according to the quantile range to normalize the price data \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors\n",
    "from keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers\n",
    "from keras.callbacks import EarlyStopping # EarlyStopping during model training\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Set Connecion\n",
    "\n",
    "#Get from developers.twitter.com/App->Setting->keys&tokens\n",
    "#Just assign the credentials\n",
    "\n",

    "\n",
    "consumer = oauth.Consumer(key=consumer_key, secret=consumer_secret)\n",
    "access_token_auth = oauth.Token(key=access_token, secret=access_token_secret)\n",
    "client = oauth.Client(consumer, access_token_auth)\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token , access_token_secret )\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_table(start_time = '2022-01-18T00:00:00.000Z'):\n",
    "    today = datetime.datetime.now().date()\n",
    "    index = pd.date_range(start = dateutil.parser.isoparse(start_time).date(), end = today, freq='D')\n",
    "    columns = ['Twitter_Sentiment', 'Tweets_Number', 'Tweets_Acceleration',  'Volume', 'Price', '3_Day_MA', '5_Day_MA', '8_Day_MA','Tweets_Polyfit','Sentiment_Polyfit', 'Volume_Polyfit', \"Price_Open\"]\n",
    "    df = pd.DataFrame(index=index, columns=columns)\n",
    "    df = df.fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tweet Growth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_number(dataset, start_time = '2022-01-18T00:00:00.000Z'):\n",
    "    client = tweepy.Client(bearer_token='\n",
    "    # Replace with your own search query\n",
    "    query = 'dogecoin -is:retweet'\n",
    "    today = datetime.datetime.now()\n",
    "    start_date = dateutil.parser.isoparse(start_time)\n",
    "    delta = datetime.timedelta(days=29)\n",
    " \n",
    "    while start_date.date() <= today.date():\n",
    "        if start_date.date() + delta < today.date():\n",
    "            end_date = start_date + delta\n",
    "        else:\n",
    "            end_date = today\n",
    "        \n",
    "        counts = client.get_all_tweets_count(query=query, granularity='day',start_time = start_date, end_time = end_date)\n",
    "\n",
    "        for count in counts.data:\n",
    "            tweet_date = dateutil.parser.isoparse(count[\"start\"]).date()\n",
    "            dataset.loc[str(tweet_date), 'Tweets_Number'] = int(count[\"tweet_count\"])\n",
    "            print(str(tweet_date))\n",
    "        start_date += delta\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tweet Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Write a Function to extract tweets:\n",
    "\n",
    "def get_tweets(Topic, end):\n",
    "    sentiment_df = pd.DataFrame(columns=[\"Tweet\", \"Date\"])\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=Topic, count = 100, lang = \"en\", until = end).items(10)\n",
    "    i=0\n",
    "    for tweet in tweets:\n",
    "        sentiment_df.loc[i] = [tweet.text,(tweet.created_at).date()]\n",
    "        i = len(sentiment_df)\n",
    "    return(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_multiplier(start_time = '2022-01-01T00:00:00.000Z'):\n",
    "    client = tweepy.Client(bearer_token='\n",
    "    # Replace with your own search query\n",
    "    query = 'dogecoin -is:retweet'\n",
    "    today = datetime.datetime.now()\n",
    "    start_date = dateutil.parser.isoparse(start_time)\n",
    "\n",
    "    counts = client.get_all_tweets_count(query=query, granularity='hour',start_time = start_date, end_time = today)\n",
    "    tweets_times = pd.DataFrame(columns = ['Hour', 'Count'])\n",
    "    for count in counts.data:\n",
    "        count[\"start\"] = count[\"start\"]\n",
    "        tweet_hour = count[\"start\"][11:13]\n",
    "        tweets_times = tweets_times.append({'Hour': tweet_hour, 'Count': int(count[\"tweet_count\"])}, ignore_index=True)\n",
    "    tweets_times = tweets_times.groupby(\"Hour\").mean()\n",
    "    tweets_times = tweets_times.cumsum()\n",
    "    tweets_times[\"Multiplier\"] = tweets_times[\"Count\"][-1]/tweets_times[\"Count\"]\n",
    "    tweets_times[\"Multiplier_Date_Adjust\"] = tweets_times[\"Multiplier\"].shift(periods=-5, fill_value=1)\n",
    "    now = datetime.datetime.now()\n",
    "    hour = now.strftime('%H')\n",
    "    multiplier = tweets_times[\"Multiplier\"][hour]\n",
    "    multiplier_adj = tweets_times[\"Multiplier_Date_Adjust\"][hour]\n",
    "\n",
    "    return multiplier, multiplier_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Clean the Tweet.\n",
    "\n",
    "def clean(tweet):\n",
    "    return ' '.join(re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([RT])', ' ', str(tweet).lower()).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton to analyze Sentiment\n",
    "def analyze_sentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_sentiment(dataset, Topic=\"Dogecoin -is:retweet\", end = \"2022-01-24\"):\n",
    "    df = get_tweets(Topic, end)\n",
    "    df['clean'] = df['Tweet'].apply(lambda x : clean(x))\n",
    "    df[\"Sentiment\"] = df[\"Tweet\"].apply(lambda x : analyze_sentiment(x))\n",
    "    Sentiment = df[[\"Sentiment\", \"Date\"]]\n",
    "    Daily_Sentiment = Sentiment.groupby(Sentiment['Date']).mean().reset_index()\n",
    "    \n",
    "    for i in range(len(Daily_Sentiment)):\n",
    "        dataset.loc[str(Daily_Sentiment[\"Date\"][i]), 'Twitter_Sentiment'] = Daily_Sentiment[\"Sentiment\"][i]\n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crypto_prices(dataset, start, end):\n",
    "    api_key = ,
    "    api_url = f'https://api.polygon.io/v2/aggs/ticker/X:DOGEUSD/range/1/day/{start}/{end}?unadjusted=true&sort=asc&apiKey={api_key}'\n",
    "    raw = requests.get(api_url).json()\n",
    "    df = pd.DataFrame(raw['results'])[['t','c', 'v']]\n",
    "    df = df.rename(columns = {'t':'Date','c':'Price', 'v':'Volume'})\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], unit = 'ms')\n",
    "    last_day_update = df.iloc[-1]['Price']\n",
    "    for i in range(len(df)):\n",
    "        dataset.loc[str(df[\"Date\"][i]), 'Volume'] = df[\"Volume\"][i]\n",
    "        dataset.loc[str(df[\"Date\"][i]), 'Price'] = df[\"Price\"][i]\n",
    "    return dataset, last_day_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function Tie It All Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset= pd.read_csv(\"Check.csv\", index_col=\"Date\")\n",
    "#dataset['Date'] = dataset['Date'].apply(lambda x: int(datetime.datetime.fromisoformat(x).timestamp()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Scaling & Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Prep(dataset):\n",
    "    training_dataset = dataset.copy()\n",
    "    datetime_column = training_dataset.index\n",
    "    training_dataset = training_dataset.reset_index(drop=True).copy()\n",
    "\n",
    "    FEATURES = ['Twitter_Sentiment', 'Tweets_Number', 'Tweets_Acceleration', 'Volume', '3_Day_MA', '5_Day_MA', '8_Day_MA','Tweets_Polyfit','Sentiment_Polyfit', 'Volume_Polyfit', \"Price_Open\", \"Percent_Change\"]\n",
    "     subdata= pd.DataFrame(training_dataset)\n",
    "    data_filtered = training_dataset.copy()\n",
    "    data_filtered_ext = data_filtered[FEATURES]\n",
    "    data_filtered_ext['Prediction'] = data_filtered_ext[\"Percent_Change\"]\n",
    "\n",
    "    nrows = data_filtered.shape[0]\n",
    "    np_data_unscaled = np.array(training_dataset)\n",
    "    np_ subdata = np.reshape(np_data_unscaled, (nrows, -1))\n",
    "    \n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
    "\n",
    "    scaler_pred = MinMaxScaler()\n",
    "    df_Close = pd.DataFrame(data_filtered_ext['Prediction'])\n",
    "    np_Close_scaled = scaler_pred.fit_transform(df_Close)\n",
    "\n",
    "    sequence_length = 50\n",
    "\n",
    "    def partition_dataset(sequence_length, data):\n",
    "        x, y = [], []\n",
    "        data_len = data.shape[0]\n",
    "        for i in range(sequence_length, data_len):\n",
    "            x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
    "            y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
    "        \n",
    "        # Convert the x and y to numpy arrays\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        return x, y\n",
    "\n",
    "    # Prediction Index\n",
    "    index_Close = data.columns.get_loc(\"Percent_Change\")\n",
    "\n",
    "    # Split the training data into train and train data sets\n",
    "    # As a first step, we get the number of rows to train the model on 80% of the data \n",
    "    train_data_len = math.ceil(np_data_scaled.shape[0] * .8)\n",
    "\n",
    "    # Create the training and test data\n",
    "    train_data = np_data_scaled[0:train_data_len, :]\n",
    "    test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
    "\n",
    "    # Generate training data and test data\n",
    "    x_train, y_train = partition_dataset(sequence_length, train_data)\n",
    "    x_test, y_test = partition_dataset(sequence_length, test_data)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, datetime_column, scaler, scaler_pred, train_data_len, data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Model(x_train):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\n",
    "    n_neurons = x_train.shape[1] * x_train.shape[2]\n",
    "    print(n_neurons, x_train.shape[1], x_train.shape[2])\n",
    "    model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) \n",
    "    model.add(LSTM(n_neurons, return_sequences=False))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Train(x_train, y_train, x_test, y_test, model):\n",
    "    epochs = 20\n",
    "    batch_size = 16\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test)\n",
    "                    )\n",
    "    return history, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "def LSTM_history(history, epochs):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), sharex=True)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.title(\"Model loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "    plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Test(x_test, y_test, scaler_pred, model):\n",
    "    # Get the predicted values\n",
    "    y_pred_scaled = model.predict(x_test)\n",
    "\n",
    "    # Unscale the predicted values\n",
    "    y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "    y_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
    "    print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "    print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
    "\n",
    "    # Median Absolute Percentage Error (MDAPE)\n",
    "    MDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\n",
    "    print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Model(y_pred, datetime_column, train_data_len, data_filtered):\n",
    "    # The date from which on the date is displayed\n",
    "    display_start_date = pd.Timestamp('today') - timedelta(days=365)\n",
    "\n",
    "    # Add the date column\n",
    "    data_filtered_sub = data_filtered.copy()\n",
    "    data_filtered_sub['Date'] = datetime_column\n",
    "    data_filtered_sub[\"Date\"] = pd.to_datetime(data_filtered_sub[\"Date\"], format = '%Y-%m-%d')\n",
    "\n",
    "    # Add the difference between the valid and predicted prices\n",
    "    train = data_filtered_sub[:train_data_len + 1]\n",
    "    valid = data_filtered_sub[train_data_len:]\n",
    "    valid.insert(1, \"Prediction\", y_pred.ravel(), True)\n",
    "    valid.insert(1, \"Difference\", (valid[\"Price\"]* (valid[\"Prediction\"])), True)\n",
    "    valid.insert(1, \"Predicted_Price\", (valid[\"Price\"]* (1 + valid[\"Prediction\"])), True)\n",
    "\n",
    "    # Zoom in to a closer timeframe\n",
    "    valid = valid[valid['Date'] > display_start_date]\n",
    "    train = train[train['Date'] > display_start_date]\n",
    "    valid.to_csv(\"Prediction.csv\")\n",
    "\n",
    "    # Visualize the data\n",
    "    fig, ax1 = plt.subplots(figsize=(22, 10), sharex=True)\n",
    "    xt = train['Date']; yt = train[[\"Price\"]]\n",
    "    xv = valid['Date']; yv = valid[[\"Price\", \"Predicted_Price\"]]\n",
    "    plt.title(\"Predictions vs Actual Values\", fontsize=20)\n",
    "    plt.ylabel(\"Doge\", fontsize=18)\n",
    "    plt.plot(xt, yt, color=\"#039dfc\", linewidth=2.0)\n",
    "    plt.plot(xv, yv[\"Predicted_Price\"], color=\"#E91D9E\", linewidth=2.0)\n",
    "    plt.plot(xv, yv[\"Price\"], color=\"black\", linewidth=2.0)\n",
    "    plt.legend([\"Train\", \"Test Predictions\", \"Actual Values\"], loc=\"upper left\")\n",
    "\n",
    "    # # Create the bar plot with the differences\n",
    "    x = valid['Date']\n",
    "    y = valid[\"Difference\"]\n",
    "\n",
    "    plt.bar(x, y, width=0.8, color=\"blue\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict_Tomorrow(dataset, scaler, model, scaler_pred):\n",
    "    sequence_length = 50\n",
    "    df_temp = dataset[-sequence_length:]\n",
    "    new_df = df_temp\n",
    "\n",
    "    N = sequence_length\n",
    "\n",
    "    # Get the last N day closing price values and scale the data to be values between 0 and 1\n",
    "\n",
    "    last_N_days = new_df[-sequence_length:].values\n",
    "    last_N_days_scaled = scaler.transform(last_N_days)\n",
    "\n",
    "    # Create an empty list and Append past N days\n",
    "    X_test_new = []\n",
    "    X_test_new.append(last_N_days_scaled)\n",
    "\n",
    "    # Convert the X_test data set to a numpy array and reshape the data\n",
    "    pred_price_scaled = model.predict(np.array(X_test_new))\n",
    "    pred_price_unscaled = scaler_pred.inverse_transform(pred_price_scaled.reshape(-1, 1))\n",
    "\n",
    "    # Print last price and predicted price for the next day\n",
    "    price_today = np.round(new_df['Price'][-1], 5)\n",
    "    predicted_price = price_today * (1 + np.round(pred_price_unscaled.ravel()[0], 5))\n",
    "    change_percent = np.round(100 - (price_today * 100)/predicted_price, 2)\n",
    "\n",
    "    plus = '+'; minus = ''\n",
    "    print(f'The close price for \"Dogecoin\" at {datetime.date.today()} was {price_today}')\n",
    "    print(f'The predicted close price is {predicted_price} ({plus if change_percent > 0 else minus}{change_percent}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #\n",
    "    start = \"2022-01-19\"\n",
    "    end = \"2022-01-25\"\n",
    "    #\n",
    "    df =  pd.read_csv(\"Check.csv\", index_col=\"Date\")\n",
    "    #\n",
    "    dataset = []\n",
    "    dataset = new_table(start_time = f'{start}T00:00:00.000Z')\n",
    "    dataset = tweet_number(dataset, start_time = f'{start}T00:00:00.000Z')\n",
    "    dataset = twitter_sentiment(dataset, Topic=\"Dogecoin -is:retweet\", end = end)\n",
    "    dataset, last_day_update = get_crypto_prices(dataset , start, end)\n",
    "    #\n",
    "    multiplier, multiplier_adj = growth_multiplier(start_time = '2022-01-01T00:00:00.000Z')\n",
    "    dataset.at[str(dataset.index[-1]), \"Tweets_Number\"] = dataset.iloc[-1]['Tweets_Number']* multiplier_adj\n",
    "    dataset.at[str(dataset.index[-1]), \"Volume\"] = dataset.iloc[-1]['Tweets_Number']* multiplier\n",
    "    #fix end of date pricing\n",
    "    \n",
    "    #\n",
    "    dataset[\"Twitter_Sentiment\"] = 1\n",
    "    #\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.append(dataset)\n",
    "    dataset = df\n",
    "    if dataset.iloc[-2]['Price'] == dataset.iloc[-3]['Price']:\n",
    "        dataset.at[str(dataset.index[-2]), \"Price\"] = last_day_update\n",
    "    dataset.at[str(dataset.index[-1]), \"Price\"] = dataset.iloc[-2]['Price']\n",
    "    dataset['3_Day_MA'] = dataset['Price'].rolling(3).mean()\n",
    "    dataset['5_Day_MA'] = dataset['Price'].rolling(5).mean()\n",
    "    dataset['8_Day_MA'] = dataset['Price'].rolling(8).mean()\n",
    "    for i in range(len(dataset)-1):\n",
    "        dataset.at[str(dataset.index[i+1]), 'Tweets_Acceleration'] = ((dataset.iloc[i+1]['Tweets_Number']-dataset.iloc[i]['Tweets_Number'])/dataset.iloc[i]['Tweets_Number'])\n",
    "        dataset.at[str(dataset.index[i+1]), \"Price_Open\"] = (float(dataset.iloc[i]['Price'])+.001)\n",
    "        if i >= 2:\n",
    "            dataset.at[str(dataset.index[i+1]), \"Percent_Change\"] = (dataset.iloc[i]['Price']-dataset.iloc[i]['Price_Open'])/dataset.iloc[i]['Price_Open']\n",
    "        if i >= 7:\n",
    "            poly_fit_number = np.poly1d(np.polyfit(np.arange(7), dataset[\"Tweets_Number\"][i-7:i].to_numpy(), 2))\n",
    "            poly_fit_sent = np.poly1d(np.polyfit(np.arange(7),dataset[\"Twitter_Sentiment\"][i-7:i].to_numpy(), 2))\n",
    "            poly_fit_volume = np.poly1d(np.polyfit(np.arange(7), dataset[\"Volume\"][i-7:i].to_numpy(), 2))\n",
    "            dataset.loc[str(dataset.index[i+1]), \"Tweets_Polyfit\"] = np.polyval(poly_fit_number, 7)\n",
    "            dataset.loc[str(dataset.index[i+1]), \"Sentiment_Polyfit\"] = np.polyval(poly_fit_sent, 7)\n",
    "            dataset.loc[str(dataset.index[i+1]), \"Volume_Polyfit\"] = np.polyval(poly_fit_volume, 7)\n",
    "    dataset.index.name = \"Date\"\n",
    "    dataset.reset_index(drop = False, inplace = True)\n",
    "    dataset[\"Date\"] = dataset[\"Date\"].astype(str)\n",
    "    dataset.set_index(\"Date\", inplace= True)\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset.to_csv(\"Check.csv\")\n",
    "    x_train, y_train, x_test, y_test, datetime_column, scaler, scaler_pred, train_data_len, data_filtered = LSTM_Prep(dataset)\n",
    "    model = LSTM_Model(x_train)\n",
    "    history, epochs = LSTM_Train(x_train, y_train, x_test, y_test, model)\n",
    "    LSTM_history(history, epochs)\n",
    "    y_pred = LSTM_Test(x_test, y_test, scaler_pred, model)\n",
    "    Plot_Model(y_pred, datetime_column, train_data_len, data_filtered)\n",
    "    Predict_Tomorrow(dataset, scaler, model, scaler_pred)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19\n",
      "2022-01-20\n",
      "2022-01-21\n",
      "2022-01-22\n",
      "2022-01-23\n",
      "2022-01-24\n",
      "2022-01-25\n",
      "650 50 13\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4q/f_kkxw0n5fgfb1tpzb6dw3mr0000gn/T/ipykernel_7248/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4q/f_kkxw0n5fgfb1tpzb6dw3mr0000gn/T/ipykernel_7248/3906275706.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Prep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mLSTM_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4q/f_kkxw0n5fgfb1tpzb6dw3mr0000gn/T/ipykernel_7248/3111168299.py\u001b[0m in \u001b[0;36mLSTM_Train\u001b[0;34m(x_train, y_train, x_test, y_test, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     history = model.fit(x_train, y_train, \n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
