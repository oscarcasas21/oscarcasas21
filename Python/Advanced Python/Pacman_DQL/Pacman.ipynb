{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5a096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import collections\n",
    "import gym\n",
    "from gym import envs\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_agents\n",
    "from tf_agents.environments import suite_atari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c1fc66",
   "metadata": {},
   "source": [
    "## Deep Q-Learning in the OpenAI Gym\n",
    "Q-Learning is a robust machine learning algorithm. Unfortunately, Q-Learning requires that the Q-table contain an entry for every possible state that the environment can take. If the environment only includes a handful of discrete state elements, then traditional Q-learning might be a good learning algorithm. However, if the state space is large, the Q-table can become prohibitively large.\n",
    "Creating policies for large state spaces is a task that Deep Q-Learning Networks (DQN) can usually handle. Unlike a table, a neural network does not require the program to represent every combination of state and action. Neural networks can generalize these states and learn commonalities. A DQN maps the state to its input neurons and the action Q-values to the output neurons. The DQN effectively becomes a function that accepts state and suggestions an action by returning the expected reward for each of the possible actions. Figure 1.DQL demonstrates the DQN structure and mapping between state and action.\n",
    "\n",
    "Figure 1.DQL: Deep Q-Learning (DQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4252f4",
   "metadata": {},
   "source": [
    "![image info](./img/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1dc670",
   "metadata": {},
   "source": [
    "As this diagram illustrates, the environment state contains several elements.  We can refer DQL as Deep Q-Network (DQN). For the basic DQN the state can be a mix of continuous and categorical/discrete values.  For the DQN, the discrete state elements the program typically encoded as dummy variables. The actions should be discrete when your program implements a DQN.  Other algorithms support continuous outputs, which we will discuss later in this chapter.\n",
    "\n",
    "In this Excecise, we will make use of [TF-Agents](https://www.tensorflow.org/agents) to implement a DQN to solve the MountainCar environment.  TF-Agents makes designing, implementing, and testing new RL algorithms easier by providing well tested modular components that can be modified and extended. It enables fast code iteration, with functional test integration and benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80913021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "#display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394af90",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Several hyperparameters must be defined. The TF-Agent example provided reasonably well-tuned hyperparameters for atari. These hyperparameters are tuned for Pacman and took the bulk of the time in creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd890422",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_collect_steps = 200\n",
    "gamma = 0.99 #discount factor\n",
    "\n",
    "batch_size =   32\n",
    "log_interval =   5000\n",
    "\n",
    "num_eval_episodes = 1\n",
    "eval_interval = 25000  \n",
    "\n",
    "max_episode_frames=108000\n",
    "ATARI_FRAME_SKIP = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9172b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brew install cmake zlib\n",
    "# pip install 'gym[atari]'\n",
    "# pip install 'gym[accept-rom-license]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ccceb",
   "metadata": {},
   "source": [
    "Here we will collect the observations such as images in this case and divide those observation (frames or images) which are the RGB images to 255 and convert into greyscale since RGB images do not help in the learning aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e305919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationCollector(gym.Wrapper):\n",
    "\n",
    "  def __init__(self, env):\n",
    "    super(ObservationCollector, self).__init__(env)\n",
    "    self._observations = collections.deque(maxlen=50000)\n",
    "    \n",
    "  def step(self, action):\n",
    "    observation, accumulated_reward, is_terminal, info = self.env.step(action)\n",
    "    self._observations.append(observation) \n",
    "    return observation, accumulated_reward, is_terminal, info\n",
    "  \n",
    "  def reset(self):\n",
    "    observation = self.env.reset()\n",
    "    self._observations.clear()\n",
    "    self._observations.append(observation)\n",
    "    return observation\n",
    "  \n",
    "  def return_observations(self):\n",
    "    return self._observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b09027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "\n",
    "  def __init__(self, env):\n",
    "    super(Normalizer, self).__init__(env)\n",
    "    self._env = env\n",
    "    self._observation_spec = tf_agents.specs.BoundedArraySpec(\n",
    "        shape = env.observation_spec().shape,\n",
    "        dtype = np.float32,\n",
    "        minimum = 0.0,\n",
    "        maximum = 1.0,\n",
    "        name = env.observation_spec().name)\n",
    "    \n",
    "  def _step(self, action):\n",
    "    time_step = self._env.step(action)  \n",
    "    observation = time_step.observation.astype('float32')\n",
    "    time_step = time_step._replace(observation = observation/255.0)\n",
    "    return time_step\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "  \n",
    "  def _reset(self):\n",
    "    time_step = self._env.reset()\n",
    "    observation = time_step.observation.astype('float32')\n",
    "    time_step = time_step._replace(observation = observation/255.0)\n",
    "    return time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd26efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which environment or game we want to play\n",
    "environment_name = \"MsPacman-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157283a",
   "metadata": {},
   "source": [
    "# test and train environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e05c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_atari.load(\n",
    "    environment_name, \n",
    "    gym_env_wrappers = suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING, \n",
    "    env_wrappers=(Normalizer,))\n",
    "\n",
    "test_py_env = suite_atari.load(\n",
    "    environment_name,\n",
    "    gym_env_wrappers = (ObservationCollector,) + suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING, \n",
    "    env_wrappers = (Normalizer,))\n",
    "\n",
    "train_tf_env = tf_agents.environments.tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "test_tf_env = tf_agents.environments.tf_py_environment.TFPyEnvironment(test_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5476bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define the agent which is DQN in this case\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "q_net = tf_agents.networks.q_network.QNetwork(\n",
    "    input_tensor_spec = train_tf_env.observation_spec(),\n",
    "    action_spec = train_tf_env.action_spec(),\n",
    "    conv_layer_params = ((32, 8, 4), (64, 4, 2), (64, 3, 1)), \n",
    "    fc_layer_params = (512,))\n",
    "\n",
    "agent = tf_agents.agents.DqnAgent(\n",
    "    train_tf_env.time_step_spec(),\n",
    "    train_tf_env.action_spec(),\n",
    "    q_network = q_net,\n",
    "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0003),\n",
    "    epsilon_greedy = 0.03,\n",
    "    n_step_update = 2,\n",
    "    target_update_tau = 0.005,\n",
    "    td_errors_loss_fn = tf_agents.utils.common.element_wise_huber_loss,\n",
    "    gamma = 0.99,\n",
    "    train_step_counter = global_step)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c48688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:383: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "# replay buffer to store the observations which will agent uses to learn the environmet\n",
    "replay_buffer = tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = agent.collect_data_spec,\n",
    "    batch_size = train_tf_env.batch_size,\n",
    "    max_length = 10000)\n",
    "\n",
    "dataset = replay_buffer.as_dataset(sample_batch_size = 64, num_steps = 3, num_parallel_calls = 3).prefetch(3)\n",
    "dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0229f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_episodes_metric = tf_agents.metrics.tf_metrics.NumberOfEpisodes()\n",
    "average_return_metric = tf_agents.metrics.tf_metrics.AverageReturnMetric()\n",
    "random_policy = tf_agents.policies.random_tf_policy.RandomTFPolicy(train_tf_env.time_step_spec(), train_tf_env.action_spec())\n",
    "\n",
    "train_driver = tf_agents.drivers.dynamic_step_driver.DynamicStepDriver(\n",
    "    env = train_tf_env,\n",
    "    policy = agent.collect_policy,\n",
    "    observers = [replay_buffer.add_batch, number_episodes_metric],\n",
    "    num_steps = 10)\n",
    "\n",
    "test_driver = tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    env = test_tf_env,\n",
    "    policy = agent.policy,\n",
    "    observers = [average_return_metric],\n",
    "    num_episodes = 2)\n",
    "\n",
    "random_driver = tf_agents.drivers.dynamic_step_driver.DynamicStepDriver(\n",
    "    env = train_tf_env,\n",
    "    policy = random_policy,\n",
    "    observers = [replay_buffer.add_batch],\n",
    "    num_steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc13fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(os.getcwd(), 'checkpoint')\n",
    "\n",
    "train_checkpointer = tf_agents.utils.common.Checkpointer(\n",
    "    ckpt_dir = checkpoint_dir,\n",
    "    max_to_keep = 1,\n",
    "    agent = agent,\n",
    "    policy = agent.policy,\n",
    "    replay_buffer = replay_buffer,\n",
    "    global_step = global_step)\n",
    "\n",
    "train_checkpointer.initialize_or_restore()\n",
    "global_step = tf.compat.v1.train.get_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ca622c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to store videos\n",
    "def capture_episodes(video_filename, num_episodes = 5):\n",
    "    with imageio.get_writer(video_filename, fps = 30) as video:\n",
    "      for _ in range(num_episodes):\n",
    "        time_step = test_tf_env.reset()\n",
    "        while not time_step.is_last():\n",
    "          policy_step = agent.policy.action(time_step)\n",
    "          time_step = test_tf_env.step(policy_step.action)\n",
    "\n",
    "        for observation in test_py_env.return_observations():\n",
    "          video.append_data(observation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff8ccf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_step == 0:\n",
    "  random_driver.run()\n",
    "  os.makedirs('videos')   \n",
    "  capture_episodes('videos/no_training.mp4')\n",
    "  \n",
    "step = global_step\n",
    "time_step = train_tf_env.reset()\n",
    "agent.train = tf_agents.utils.common.function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae6ef3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1096: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4q/f_kkxw0n5fgfb1tpzb6dw3mr0000gn/T/ipykernel_98660/3348917905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m  \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# running an agent\n",
    "# note that this will take quite a long time.\n",
    "for epoch in range(step + 1, step + 100001):\n",
    "  time_step, _ = train_driver.run(time_step)\n",
    "  experience, _ = next(dataset)\n",
    "  loss, _ = agent.train(experience)\n",
    "\n",
    "  if epoch % 1000  == 0:\n",
    "    test_driver.run()\n",
    "    num_episodes = number_episodes_metric.result().numpy()\n",
    "    test_score = average_return_metric.result().numpy() \n",
    "    average_return_metric.reset()\n",
    "    capture_episodes(f'videos/epoch_{epoch}_episode_{num_episodes+6848}_score_{test_score}.mp4')\n",
    "    train_checkpointer.save(global_step)\n",
    "    step = epoch\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
