{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc72613e",
   "metadata": {},
   "source": [
    "# Docker Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31ede24",
   "metadata": {},
   "source": [
    "Here is the whole process of my project with all the used code included without all the mistakes and noise that is included in the history.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0277d6",
   "metadata": {},
   "source": [
    "## Create Docker-Compose.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create file\n",
    "touch docker-compose.ysl\n",
    "\n",
    "#edit file\n",
    "vi docker-compose.ysl\n",
    "\n",
    "#feel free to bring in images once created and edited (or copy paste a old file and then do pulls.\n",
    "\n",
    "docker pull midsw205/base:latest\n",
    "docker pull midsw205/base:0.1.8\n",
    "docker pull midsw205/base:0.1.9\n",
    "docker pull redis\n",
    "docker pull confluentinc/cp-zookeeper:latest\n",
    "docker pull confluentinc/cp-kafka:latest\n",
    "docker pull midsw205/spark-python:0.0.5\n",
    "docker pull midsw205/spark-python:0.0.6\n",
    "docker pull midsw205/cdh-minimal:latest\n",
    "docker pull midsw205/hadoop:0.0.2\n",
    "docker pull midsw205/presto:0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed083d",
   "metadata": {},
   "source": [
    "## Docker Pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make directory\n",
    "mkdir ~/w205/project-2-oscarcasas\n",
    "\n",
    "#cd into directory\n",
    "cd ~/w205/project-2-oscarcasas\n",
    "\n",
    "#add datafile\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "\n",
    "#Prune network, if still not removed reset instance\n",
    "docker network ls\n",
    "\n",
    "#check network\n",
    "docker network ls\n",
    "\n",
    "#spin dicker up\n",
    "docker-compose up -d\n",
    "\n",
    "RETURN**\n",
    "Creating network \"project-2-oscarcasas_default\" with the default driver\n",
    "Creating project-2-oscarcasas_zookeeper_1 ... done\n",
    "Creating project-2-oscarcasas_cloudera_1  ... done\n",
    "Creating project-2-oscarcasas_mids_1      ... done\n",
    "Creating project-2-oscarcasas_spark_1     ... done\n",
    "Creating project-2-oscarcasas_kafka_1     ... done\n",
    "\n",
    "#check containers\n",
    "docker-compose ps\n",
    "\n",
    "RETURN**\n",
    "project-2-oscarcasas_cloudera_1    cdh_startup_script.sh       Up      11000/tcp, 11443/tcp, 19888/tcp, 50070/tcp, 8020/tcp,    \n",
    "                                                                       8088/tcp, 8888/tcp, 9090/tcp                             \n",
    "project-2-oscarcasas_kafka_1       /etc/confluent/docker/run   Up      29092/tcp, 9092/tcp                                      \n",
    "project-2-oscarcasas_mids_1        /bin/bash                   Up      8888/tcp                                                 \n",
    "project-2-oscarcasas_spark_1       docker-entrypoint.sh bash   Up                                                               \n",
    "project-2-oscarcasas_zookeeper_1   /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 32181/tcp, 3888/tcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033338e5",
   "metadata": {},
   "source": [
    "## Kafka | Zookeeper | Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907974d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check logs (a lot of logs)\n",
    "docker-compose logs -f kafka\n",
    "\n",
    "#Check hadoop\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "\n",
    "RETURN**\n",
    "Found 2 items\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-05 04:51 /tmp/hive\n",
    "\n",
    "#Create topic Exams from data\n",
    "docker-compose exec kafka kafka-topics --create --topic Exams --partitions 1 --replication-factor 1 --if-not-exists --bootstrap-server localhost:29092\n",
    "\n",
    "RETURNS**\n",
    "Created topic Exams.\n",
    "\n",
    "#Publish messages to kafka via kafkacat no limiter on number\n",
    "docker-compose exec mids bash -c \"cat assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t Exams\"\n",
    "\n",
    "#Consume the messages \n",
    "docker-compose exec mids bash -c \"kafkacat -b kafka:29092 -t Exams\"\n",
    "\n",
    "RESULT**\n",
    "Whole JSON File\n",
    "\n",
    "# Call pyspark off docker container\n",
    "docker-compose exec spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f13075",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b38c857",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2677391188.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_11038/2677391188.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    raw_exams = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", /\u001b[0m\n\u001b[0m                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#read in kafka messages that were consumed by Exams topic\n",
    "raw_exams = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", /\n",
    "\"kafka:29092\").option(\"subscribe\",\"Exams\").option(\"startingOffsets\", /\n",
    "\"earliest\").option(\"endingOffsets\", \"latest\").load()\n",
    "\n",
    "#cache it\n",
    "raw_exams.cache()\n",
    "\n",
    "#investigate schema\n",
    "raw_exams.printSchema()\n",
    "\n",
    "#Cast each message as a string\n",
    "exams = raw_exams.select(raw_exams.value.cast('string'))\n",
    "\n",
    "#RETURNS**\n",
    "#DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
    "\n",
    "#write to parquet\n",
    "exams.write.parquet(\"/tmp/exams\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b706f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from another terminal window look at it on hadoop\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "\n",
    "RESULTS***\n",
    "\n",
    "Found 3 items\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-05 05:07 /tmp/exams\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-05 04:51 /tmp/hive\n",
    "    \n",
    "\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/exams/\n",
    "\n",
    "RESULTS***\n",
    "\n",
    "Found 2 items\n",
    "-rw-r--r--   1 root supergroup          0 2021-11-05 05:07 /tmp/exams/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup    2513397 2021-11-05 05:07 /tmp/exams/part-00000-5ccf8a3d-8d71-44a0-8749-b7931511a5da-c00\n",
    "0.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc88b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (442237244.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_11038/442237244.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    RESULTS**\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#back in the original terminal\n",
    "exams.show()\n",
    "\n",
    "RESULTS**\n",
    "+--------------------+\n",
    "|               value|\n",
    "+--------------------+\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "|{\"keen_timestamp\"...|\n",
    "+--------------------+\n",
    "only showing top 20 rows\n",
    "  \n",
    "#Not useful at all need more investigating\n",
    "print(exams.head())\n",
    "  \n",
    "RESULTS***\n",
    "  \n",
    "Row(value='{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37\n",
    "f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incom\n",
    "plete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358f\n",
    "af850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\n",
    "\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":t\n",
    "rue,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"opti\n",
    "ons\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":fals\n",
    "e,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef\n",
    "8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user\n",
    "_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"\n",
    "id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"\n",
    "2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-0\n",
    "1-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8\n",
    "644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":fal\n",
    "se,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"\n",
    "id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f1\n",
    "06765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"\n",
    "}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":fa\n",
    "lse,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8\n",
    "ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}')\n",
    "\n",
    "#impor json\n",
    "import json\n",
    "\n",
    "#look at one value\n",
    "Peak = json.loads(exams.select('value').take(1)[0].value)\n",
    "    \n",
    "#investigate\n",
    "Peak['sequences']['counts']\n",
    "\n",
    "RETURNS**\n",
    "{'incomplete': 1, 'submitted': 4, 'incorrect': 1, 'all_correct': False, 'correct': 2, 'total': 4, 'unanswered': 0}\n",
    "    \n",
    "#further\n",
    "Peak['sequences']['counts']['correct']\n",
    "    \n",
    "RETURNS**\n",
    "2\n",
    "\n",
    "#Deal with unicode\n",
    "import sys\n",
    "sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf8', buffering=1)\n",
    "    \n",
    "#check\n",
    "print(sys.stdout.encoding)\n",
    "\n",
    "RETURNS**\n",
    "utf8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE:This led me through a very long journey to nowhere, I have no doubt there is a way to do it \n",
    "#I just couldn't figure it out and will reflect that in my report.\n",
    "#This solution creates a easier (slower run if big data) to interpret list of lists which can be converted to a dataframe easily\n",
    "df = pd.DataFrame(columns=['Class', 'User_id', 'Correct', 'Total'])\n",
    "for i in range(exams.count()):\n",
    "    df.loc[i] = [json.loads(exams.select('value').take(3280)[i+1].value)['exam_name'],\n",
    "               json.loads(exams.select('value').take(3280)[i+1].value)['user_exam_id'],\n",
    "               json.loads(exams.select('value').take(3280)[i+1].value)['sequences']['counts']['correct'],\n",
    "               json.loads(exams.select('value').take(3280)[i+1].value)['sequences']['counts']['total']]\n",
    "    print(i)\n",
    "\n",
    "#Make Parquet File delete\n",
    "parquet_df = spark.createDataFrame(df,[\"Class\",\"User_id\",\"Correct\",\"Total\"])\n",
    "\n",
    "#Write Parquet\n",
    "parquet_df.write.parquet(\"/tmp/pass_class\")\n",
    "\n",
    "#Investigate\n",
    "parquet_df.show()\n",
    "\n",
    "\n",
    "+--------------------+--------------------+-------+-----+\n",
    "|               Class|             User_id|Correct|Total|\n",
    "+--------------------+--------------------+-------+-----+\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "|Normal Forms and ...|6d4089e4-bde5-4a2...|      2|    4|\n",
    "+--------------------+--------------------+-------+-----+\n",
    "\n",
    "#clearly it was summing up by the wrong iterable in the json, but we can just fix that by grouping by the User_id\n",
    "\n",
    "\n",
    "parquet_df.registerTempTable('commits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d920ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "Found 5 items\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-05 05:59 /tmp/ex_exams\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-05 05:07 /tmp/exams\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-11-05 04:51 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2021-11-05 07:27 /tmp/pass_class\n",
    "jupyter@tensorflow-2-3-20210907-151006:~/w205/project-2-oscarcasas$ docker-compose exec cloudera hadoop fs -ls /tmp/p\n",
    "ass_class\n",
    "Found 5 items\n",
    "-rw-r--r--   1 root supergroup          0 2021-11-05 07:27 /tmp/pass_class/_SUCCESS\n",
    "-rw-r--r--   1 root supergroup       1441 2021-11-05 07:27 /tmp/pass_class/part-00000-e9cf588c-802c-4068-857c-1422eb3\n",
    "d7c4f-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       1441 2021-11-05 07:27 /tmp/pass_class/part-00001-e9cf588c-802c-4068-857c-1422eb3\n",
    "d7c4f-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       1441 2021-11-05 07:27 /tmp/pass_class/part-00002-e9cf588c-802c-4068-857c-1422eb3\n",
    "d7c4f-c000.snappy.parquet\n",
    "-rw-r--r--   1 root supergroup       1441 2021-11-05 07:27 /tmp/pass_class/part-00003-e9cf588c-802c-4068-857c-1422eb3\n",
    "d7c4f-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "exams.rdd.map(lambda x: json.loads(exams.select('value').take(x)[0].value)['exam_name']).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "exams.rdd.map(lambda x: json.loads(x.value)).toDF().show()\n",
    "\n",
    "ex_exams = exams.rdd.map(lambda x: json.loads(x.value)).toDF()\n",
    "\n",
    "#output big dataframes with a lot of information we dont need\n",
    "\n",
    "ex_exams.printSchema()\n",
    "\n",
    "RESULT**\n",
    "\n",
    "|-- base_exam_id: string (nullable = true)\n",
    " |-- certification: string (nullable = true)\n",
    " |-- exam_name: string (nullable = true)\n",
    " |-- keen_created_at: string (nullable = true)\n",
    " |-- keen_id: string (nullable = true)\n",
    " |-- keen_timestamp: string (nullable = true)\n",
    " |-- max_attempts: string (nullable = true)\n",
    " |-- sequences: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: array (valueContainsNull = true)\n",
    " |    |    |-- element: map (containsNull = true)\n",
    " |    |    |    |-- key: string\n",
    " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
    " |-- started_at: string (nullable = true)\n",
    " |-- user_exam_id: string (nullable = true)\n",
    "\n",
    "#save to parquet\n",
    "ex_exams.write.parquet(\"/tmp/ex_exams\")\n",
    "\n",
    "#Registed the table \n",
    "ex_exams.registerTempTable('exam_com')\n",
    "\n",
    "#after all queries are coded exit pyspark and close docker-compose\n",
    "exit()\n",
    "\n",
    "docker-compose down"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
