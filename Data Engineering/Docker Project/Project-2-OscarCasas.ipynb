{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Oscar Casas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary, Assumptions and Thought Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##         1) Summary of Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Docker-Compose.yml file was created in the github project repository clustered to take in hadoop, spark, mids, redis, kafka and zookeeper. \n",
    "\n",
    "b) Json file is called with curl function from its original location api to server and put on working server. \n",
    "\n",
    "c) Kafka then creates a topic exams where it then the json file messages are published and Consumed.\n",
    "\n",
    "d) Docker-Compose is then spun up and a new topic is created with the kafka server to be used to store events. Since Kafka is a distributed system, with a lot of data, it can be partitioned and replicated across multiple nodes for security and speed. For this assignment the data is only in 1 partition and is replicated once.\n",
    "\n",
    "e) Pyspark is then spun up through Docker spark container, where pyspark is then used to read the json file from kafka.\n",
    "\n",
    "g) Pyspark then writes first pass to HDFS parquet.\n",
    "\n",
    "h) Back in Pyspark call first pass from HDFS and use Spark SQL to grab what we want and answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) 1 partition and 1 replication will be enough to handle this data file and transformation.\n",
    "\n",
    "b) The data was not distrupted in any of the processes even though only one replication.\n",
    "\n",
    "c) The data is a representation of exams to different students in different classes and all tests are multiple choice.\n",
    "\n",
    "d) False on question means not filled in, True means filled in.\n",
    "\n",
    "e) All assumptions on variable names meaning on data section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Docker-Compose.yml Cluster: While I created it myself, I called it directly from the same source as the source from the synch material. In reality I would have to call each one individually from a different source.\n",
    "\n",
    "b) Due to the fact that we have to do it in GCP there were a couple processes that might have been simplified for this data source if done elsewhere. \n",
    "\n",
    "c) Instructions for how to partition/replicate data, we were told in class to do 1 and 1 respectively. Which makes perfect sense for this data as it is not too large and data loss is not a primary issue.\n",
    "\n",
    "d) We might have been able to find the same answers more efficiently to questions if we had just used advanced level jq in docker and simplified the pipeline. That being said for most of the class including me being able to use pyspark definitely was great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Self Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The actual business questions where individually chosen.\n",
    "\n",
    "b) The data wrangling to be done on either pyspark in SQL or python.\n",
    "\n",
    "c) The actual steps in the pipeline, while I guess a lot of us did similar things, we had open range to do what we felt worked best.\n",
    "\n",
    "d) The data assumptions for what everything meant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) What Parts of the Pipeline Do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Docker - creates the cluster of containers to work in its environment\n",
    "\n",
    "b) Kafka - used to create topic, the name chosen was exams because from the data it was clear some type of assessments were taking place.\n",
    "\n",
    "c) spark - Is able to take the JSON data Kafka outputs and extract it as well as modify it.\n",
    "\n",
    "f) Hadoop - used to store the data outputted from spark do it can be readily available for other users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Data Issues & Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data issues were vast specifically because it took a very long time for me to understand what the data even was. Once I was able to pretty print it, I still wasnt able to understand the data at all, I thought it was all errors because there was false/correct right next to eachother. That was until I figured out it was a multiple choice test that for each question there were usually 4 possible answers and the true and false meant that they had filled that asnwer in. Once I realized that, the nested function became a lot easier to deal with. The fact that it was a nested function really was annoying especially because it was nested in multiple areas sometimes where it wasnt even necessary. It took me a very long time to fix the nested function to compare grades but in the end I got it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here are my assumptions regarding the dataset:\n",
    "    \n",
    "base_exam_id: The specific exam identifier.\n",
    "certification: Whether passing the exam gave a certification.\n",
    "exam_name: The name of the exam.\n",
    "keen_created_at: The time the assessment was created for the test takers\n",
    "keen_id: id of keen, could be directed to user\n",
    "keen_timestamp: time of test potentially\n",
    "max_attempts: How many attempts each user has\n",
    "sequences: Structure containing sub variables\n",
    "    attempt: number of attempts\n",
    "    counts: nested structure:\n",
    "        all_correct: if all questions answered correctly true\n",
    "        correct: number correct answered\n",
    "        incomplete: number of incomplete questions\n",
    "        incorrect: number of incorrect questions\n",
    "        submitted: number of submitted questions\n",
    "        total: number of questions on exam\n",
    "        unanswered: same as incomplete\n",
    "    id: Not specific but could be a set of questions\n",
    "    questions: tracking data for each question\n",
    "        at: when submitted\n",
    "        checked: if question answered true\n",
    "        correct: if correctly answered true\n",
    "        id: question id\n",
    "        submitted: if question submitted true\n",
    "    user_correct: if all correct true\n",
    "    user_incomplete: if > 1 incomplete true\n",
    "    user_result: how many correct a user got\n",
    "    user_submitted: whether all questions were submitted\n",
    "started_at: Time exam started (lots of times, not certain of all)\n",
    "user_exam_id: Users specific exam id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) What exam was the most and least popular ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) What percent of people stay on the learning path for Git?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Find top 10 easiests exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries and Pyspark Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Investigation:\n",
    "#Select all from table\n",
    "spark.sql(\"select * from exam_com limit 10\").show()\n",
    "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequence\n",
    "s|          started_at|        user_exam_id|\n",
    "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+-------------------\n",
    "-+--------------------+--------------------+\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
    "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
    "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
    "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
    "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
    "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
    "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
    "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ..\n",
    ".|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
    "+--------------------+-------------+--------------------+------------------+-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find exam names with highest amount of entries\n",
    "spark.sql(\"select exam_name, COUNT(*) from exam_com Group By exam_name Order By Count(*) DESC \").show()\n",
    "--------------------+--------+\n",
    "|           exam_name|count(1)|\n",
    "+--------------------+--------+\n",
    "|        Learning Git|     394|\n",
    "|Introduction to P...|     162|\n",
    "|Intermediate Pyth...|     158|\n",
    "|Introduction to J...|     158|\n",
    "|Learning to Progr...|     128|\n",
    "|Introduction to M...|     119|\n",
    "|Software Architec...|     109|\n",
    "|Beginning C# Prog...|      95|\n",
    "|    Learning Eclipse|      85|\n",
    "|Learning Apache M...|      80|\n",
    "|Beginning Program...|      79|\n",
    "|       Mastering Git|      77|\n",
    "|Introduction to B...|      75|\n",
    "|Advanced Machine ...|      67|\n",
    "|Learning Linux Sy...|      59|\n",
    "|JavaScript: The G...|      58|\n",
    "|        Learning SQL|      57|\n",
    "|Practical Java Pr...|      53|\n",
    "|    HTML5 The Basics|      52|\n",
    "|   Python Epiphanies|      51|\n",
    "+--------------------+--------+\n",
    "\n",
    "#change ordering\n",
    "spark.sql(\"select exam_name, COUNT(*) from exam_com Group By exam_name Order By Count(*) ASC \").show()\n",
    "\n",
    "+--------------------+--------+\n",
    "|           exam_name|count(1)|\n",
    "+--------------------+--------+\n",
    "|Native Web Apps f...|       1|\n",
    "|Nulls, Three-valu...|       1|\n",
    "|Learning to Visua...|       1|\n",
    "|Operating Red Hat...|       1|\n",
    "|Learning Spring P...|       2|\n",
    "|Understanding the...|       2|\n",
    "|Client-Side Data ...|       2|\n",
    "|Hibernate and JPA...|       2|\n",
    "|Arduino Prototypi...|       2|\n",
    "|What's New in Jav...|       2|\n",
    "|The Closed World ...|       2|\n",
    "|Building Web Serv...|       3|\n",
    "|Using Web Components|       3|\n",
    "|Service Based Arc...|       3|\n",
    "|Getting Ready for...|       3|\n",
    "| Mastering Web Views|       3|\n",
    "|Using Storytellin...|       4|\n",
    "|       View Updating|       4|\n",
    "|Example Exam For ...|       5|\n",
    "|An Introduction t...|       5|\n",
    "+--------------------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2986496116.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_11148/2986496116.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    +--------------------+--------+\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Find all classes inclocing git and how many people took the exams\n",
    "spark.sql(\"select exam_name, COUNT(*) from exam_com WHERE exam_name LIKE '%Git%' GROUP BY exam_name\").show()\n",
    "\n",
    "+--------------------+--------+\n",
    "|           exam_name|count(1)|\n",
    "+--------------------+--------+\n",
    "|Mastering Advance...|      34|\n",
    "|        Learning Git|     394|\n",
    "|       Mastering Git|      77|\n",
    "|Collaborating wit...|       6|\n",
    "|Git Fundamentals ...|      28|\n",
    "+--------------------+--------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
